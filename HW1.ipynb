{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_priors = None\n",
        "        self.feature_means = None\n",
        "        self.feature_vars = None\n",
        "        self.classes = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Naive Bayes classifier\n",
        "\n",
        "        Parameters:\n",
        "        X: Training data features [n_samples, n_features]\n",
        "        y: Training data labels [n_samples]\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.class_priors = np.zeros(n_classes)\n",
        "        self.feature_means = np.zeros((n_classes, n_features))\n",
        "        self.feature_vars = np.zeros((n_classes, n_features))\n",
        "\n",
        "        # Calculate class priors and feature statistics for each class\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            self.class_priors[i] = X_c.shape[0] / n_samples\n",
        "            self.feature_means[i, :] = X_c.mean(axis=0)\n",
        "            self.feature_vars[i, :] = X_c.var(axis=0) + 1e-6  # Add small value to avoid zero variance\n",
        "\n",
        "    def _calculate_likelihood(self, X):\n",
        "        \"\"\"\n",
        "        Calculate likelihood of the data under each class\n",
        "\n",
        "        Parameters:\n",
        "        X: Test data features [n_samples, n_features]\n",
        "\n",
        "        Returns:\n",
        "        likelihoods: Likelihood for each sample under each class [n_samples, n_classes]\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(self.classes)\n",
        "        likelihoods = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            # Gaussian probability density\n",
        "            deviations = X - self.feature_means[i, :]\n",
        "            exponent = -0.5 * np.sum(deviations**2 / self.feature_vars[i, :], axis=1)\n",
        "            normalizer = 1 / np.sqrt((2 * np.pi) ** n_features * np.prod(self.feature_vars[i, :]))\n",
        "            likelihoods[:, i] = normalizer * np.exp(exponent)\n",
        "\n",
        "        return likelihoods\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels and calculate discriminant functions\n",
        "\n",
        "        Parameters:\n",
        "        X: Test data features [n_samples, n_features]\n",
        "\n",
        "        Returns:\n",
        "        predicted_classes: Predicted class labels [n_samples]\n",
        "        discriminant_values: Values of discriminant functions [n_samples, n_classes]\n",
        "        \"\"\"\n",
        "        likelihoods = self._calculate_likelihood(X)\n",
        "        # Calculate posterior probabilities (discriminant functions)\n",
        "        discriminant_values = likelihoods * self.class_priors\n",
        "\n",
        "        # Normalize to get proper probabilities (optional)\n",
        "        discriminant_values = discriminant_values / np.sum(discriminant_values, axis=1, keepdims=True)\n",
        "\n",
        "        # Get predicted class (maximum posterior)\n",
        "        predicted_indices = np.argmax(discriminant_values, axis=1)\n",
        "        predicted_classes = self.classes[predicted_indices]\n",
        "\n",
        "        return predicted_classes, discriminant_values\n",
        "\n",
        "class PerceptronClassifier:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        \"\"\"\n",
        "        Initialize Perceptron classifier\n",
        "\n",
        "        Parameters:\n",
        "        learning_rate: Learning rate for weight updates\n",
        "        n_iterations: Maximum number of iterations\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.classes = None\n",
        "\n",
        "    def _initialize_weights(self, n_features, n_classes):\n",
        "        \"\"\"Initialize weights and bias\"\"\"\n",
        "        if n_classes == 2:\n",
        "            # Binary classification: One set of weights\n",
        "            self.weights = np.zeros(n_features)\n",
        "            self.bias = 0\n",
        "        else:\n",
        "            # Multi-class: One set of weights per class\n",
        "            self.weights = np.zeros((n_classes, n_features))\n",
        "            self.bias = np.zeros(n_classes)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Perceptron classifier\n",
        "        Parameters:\n",
        "        X: Training data features [n_samples, n_features]\n",
        "        y: Training data labels [n_samples]\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Map class labels to integers starting from 0\n",
        "        y_mapped = np.zeros_like(y, dtype=int)\n",
        "        for i, c in enumerate(self.classes):\n",
        "            y_mapped[y == c] = i\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights(n_features, n_classes)\n",
        "\n",
        "        # Train the model\n",
        "        if n_classes == 2:\n",
        "            # Binary classification\n",
        "            for _ in range(self.n_iterations):  # Fixed: removed asterisk\n",
        "                for idx, x_i in enumerate(X):\n",
        "                    # Convert class 0 to -1 for binary classification\n",
        "                    y_i = 1 if y_mapped[idx] == 1 else -1\n",
        "\n",
        "                    # Calculate activation\n",
        "                    activation = np.dot(x_i, self.weights) + self.bias\n",
        "\n",
        "                    # Update weights if misclassified\n",
        "                    if y_i * activation <= 0:\n",
        "                        self.weights += self.learning_rate * y_i * x_i\n",
        "                        self.bias += self.learning_rate * y_i\n",
        "        else:\n",
        "            # Multi-class classification (one-vs-rest)\n",
        "            for _ in range(self.n_iterations):  # Fixed: removed asterisk\n",
        "                for idx, x_i in enumerate(X):\n",
        "                    y_true = y_mapped[idx]\n",
        "\n",
        "                    # Calculate activations for all classes\n",
        "                    activations = np.dot(x_i, self.weights.T) + self.bias  # Fixed dot product orientation\n",
        "                    y_pred = np.argmax(activations)\n",
        "\n",
        "                    # Update weights if misclassified\n",
        "                    if y_pred != y_true:\n",
        "                        self.weights[y_true] += self.learning_rate * x_i\n",
        "                        self.weights[y_pred] -= self.learning_rate * x_i\n",
        "                        self.bias[y_true] += self.learning_rate\n",
        "                        self.bias[y_pred] -= self.learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels and calculate discriminant functions\n",
        "\n",
        "        Parameters:\n",
        "        X: Test data features [n_samples, n_features]\n",
        "\n",
        "        Returns:\n",
        "        predicted_classes: Predicted class labels [n_samples]\n",
        "        discriminant_values: Values of discriminant functions [n_samples, n_classes]\n",
        "        \"\"\"\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        if n_classes == 2:\n",
        "            # Binary classification\n",
        "            discriminant_values = np.column_stack([\n",
        "                -np.dot(X, self.weights) - self.bias,  # Class 0\n",
        "                np.dot(X, self.weights) + self.bias     # Class 1\n",
        "            ])\n",
        "            predicted_indices = np.argmax(discriminant_values, axis=1)\n",
        "\n",
        "        else:\n",
        "            # Multi-class classification\n",
        "            discriminant_values = np.dot(X, self.weights.T) + self.bias\n",
        "            predicted_indices = np.argmax(discriminant_values, axis=1)\n",
        "\n",
        "        predicted_classes = self.classes[predicted_indices]\n",
        "\n",
        "        return predicted_classes, discriminant_values\n",
        "\n",
        "\n",
        "class ClassifierEvaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_confusion_matrix(self, y_true, y_pred, classes=None):\n",
        "        \"\"\"\n",
        "        Compute confusion matrix\n",
        "\n",
        "        Parameters:\n",
        "        y_true: True class labels\n",
        "        y_pred: Predicted class labels\n",
        "        classes: List of class labels (if None, will be computed from the data)\n",
        "\n",
        "        Returns:\n",
        "        confusion_matrix: Confusion matrix [n_classes, n_classes]\n",
        "        \"\"\"\n",
        "        if classes is None:\n",
        "            classes = np.unique(np.concatenate((y_true, y_pred)))\n",
        "\n",
        "        n_classes = len(classes)\n",
        "        confusion_mat = np.zeros((n_classes, n_classes), dtype=int)\n",
        "\n",
        "        for i in range(len(y_true)):\n",
        "            true_idx = np.where(classes == y_true[i])[0][0]\n",
        "            pred_idx = np.where(classes == y_pred[i])[0][0]\n",
        "            confusion_mat[true_idx, pred_idx] += 1\n",
        "\n",
        "        return confusion_mat, classes\n",
        "\n",
        "    def compute_metrics(self, confusion_matrix):\n",
        "        \"\"\"\n",
        "        Compute accuracy, precision, recall, and F1 score from confusion matrix\n",
        "\n",
        "        Parameters:\n",
        "        confusion_matrix: Confusion matrix [n_classes, n_classes]\n",
        "\n",
        "        Returns:\n",
        "        metrics_dict: Dictionary containing metrics\n",
        "        \"\"\"\n",
        "        n_classes = confusion_matrix.shape[0]\n",
        "        metrics = {}\n",
        "\n",
        "        # Overall accuracy\n",
        "        metrics['accuracy'] = np.sum(np.diag(confusion_matrix)) / np.sum(confusion_matrix)\n",
        "\n",
        "        # Per-class metrics\n",
        "        metrics['precision'] = np.zeros(n_classes)\n",
        "        metrics['recall'] = np.zeros(n_classes)\n",
        "        metrics['f1_score'] = np.zeros(n_classes)\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            # Precision\n",
        "            if np.sum(confusion_matrix[:, i]) > 0:\n",
        "                metrics['precision'][i] = confusion_matrix[i, i] / np.sum(confusion_matrix[:, i])\n",
        "            else:\n",
        "                metrics['precision'][i] = 0\n",
        "\n",
        "            # Recall\n",
        "            if np.sum(confusion_matrix[i, :]) > 0:\n",
        "                metrics['recall'][i] = confusion_matrix[i, i] / np.sum(confusion_matrix[i, :])\n",
        "            else:\n",
        "                metrics['recall'][i] = 0\n",
        "\n",
        "            # F1 score\n",
        "            if metrics['precision'][i] + metrics['recall'][i] > 0:\n",
        "                metrics['f1_score'][i] = 2 * metrics['precision'][i] * metrics['recall'][i] / (metrics['precision'][i] + metrics['recall'][i])\n",
        "            else:\n",
        "                metrics['f1_score'][i] = 0\n",
        "\n",
        "        # Macro-averaged metrics\n",
        "        metrics['macro_precision'] = np.mean(metrics['precision'])\n",
        "        metrics['macro_recall'] = np.mean(metrics['recall'])\n",
        "        metrics['macro_f1'] = np.mean(metrics['f1_score'])\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, confusion_matrix, class_names, title='Confusion Matrix'):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix\n",
        "\n",
        "        Parameters:\n",
        "        confusion_matrix: Confusion matrix [n_classes, n_classes]\n",
        "        class_names: Names of classes\n",
        "        title: Title of the plot\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(confusion_matrix, cmap='Blues')\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "\n",
        "        n_classes = len(class_names)\n",
        "        tick_marks = np.arange(n_classes)\n",
        "        plt.xticks(tick_marks, class_names, rotation=45)\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "\n",
        "        # Add text annotations\n",
        "        thresh = confusion_matrix.max() / 2\n",
        "        for i in range(n_classes):\n",
        "            for j in range(n_classes):\n",
        "                plt.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
        "                         ha=\"center\", va=\"center\",\n",
        "                         color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "        plt.ylabel('True Class')\n",
        "        plt.xlabel('Predicted Class')\n",
        "        plt.tight_layout()\n",
        "\n",
        "    def calculate_roc_curve(self, y_true_binary, discriminant_values_positive, n_points=100):\n",
        "        \"\"\"\n",
        "        Calculate ROC curve for binary classification\n",
        "\n",
        "        Parameters:\n",
        "        y_true_binary: Binary true class labels (0 or 1)\n",
        "        discriminant_values_positive: Discriminant values for the positive class\n",
        "        n_points: Number of threshold points for the ROC curve\n",
        "\n",
        "        Returns:\n",
        "        fpr_values: False positive rates\n",
        "        tpr_values: True positive rates\n",
        "        auc: Area under the ROC curve\n",
        "        \"\"\"\n",
        "        # Convert labels to binary (0 or 1)\n",
        "        y_true_binary = np.array(y_true_binary).astype(int)\n",
        "\n",
        "        # Get positive and negative scores\n",
        "        pos_scores = discriminant_values_positive[y_true_binary == 1]\n",
        "        neg_scores = discriminant_values_positive[y_true_binary == 0]\n",
        "\n",
        "        # Calculate ROC curve\n",
        "        thresholds = np.linspace(np.min(discriminant_values_positive),\n",
        "                                np.max(discriminant_values_positive), n_points)\n",
        "\n",
        "        fpr_values = []\n",
        "        tpr_values = []\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            # True positive rate\n",
        "            tp = np.sum(pos_scores >= threshold)\n",
        "            fn = np.sum(pos_scores < threshold)\n",
        "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "            # False positive rate\n",
        "            fp = np.sum(neg_scores >= threshold)\n",
        "            tn = np.sum(neg_scores < threshold)\n",
        "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "            fpr_values.append(fpr)\n",
        "            tpr_values.append(tpr)\n",
        "\n",
        "        # Calculate AUC using trapezoidal rule\n",
        "        fpr_values = np.array(fpr_values)\n",
        "        tpr_values = np.array(tpr_values)\n",
        "\n",
        "        # Sort by increasing FPR\n",
        "        sorted_indices = np.argsort(fpr_values)\n",
        "        fpr_values = fpr_values[sorted_indices]\n",
        "        tpr_values = tpr_values[sorted_indices]\n",
        "\n",
        "        # Add endpoints if needed\n",
        "        if fpr_values[0] > 0 or tpr_values[0] > 0:\n",
        "            fpr_values = np.concatenate(([0], fpr_values))\n",
        "            tpr_values = np.concatenate(([0], tpr_values))\n",
        "\n",
        "        if fpr_values[-1] < 1 or tpr_values[-1] < 1:\n",
        "            fpr_values = np.concatenate((fpr_values, [1]))\n",
        "            tpr_values = np.concatenate((tpr_values, [1]))\n",
        "\n",
        "        auc = np.trapezoid(tpr_values, fpr_values)\n",
        "\n",
        "        return fpr_values, tpr_values, auc\n",
        "\n",
        "    def plot_roc_curve(self, fpr_values, tpr_values, auc, label=None):\n",
        "        \"\"\"\n",
        "        Plot ROC curve\n",
        "\n",
        "        Parameters:\n",
        "        fpr_values: False positive rates\n",
        "        tpr_values: True positive rates\n",
        "        auc: Area under the ROC curve\n",
        "        label: Label for the curve\n",
        "        \"\"\"\n",
        "        if label is None:\n",
        "            label = f'AUC = {auc:.3f}'\n",
        "        else:\n",
        "            label = f'{label} (AUC = {auc:.3f})'\n",
        "\n",
        "        plt.plot(fpr_values, tpr_values, label=label)\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "def load_iris_dataset():\n",
        "    \"\"\"Load Iris dataset\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "    try:\n",
        "        response = urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(StringIO(data), header=None, names=column_names)\n",
        "        X = df.iloc[:, :-1].values\n",
        "        y = df.iloc[:, -1].values\n",
        "        return X, y, column_names[:-1], np.unique(y)\n",
        "    except:\n",
        "        print(\"Error loading Iris dataset from URL. Using synthetic data instead.\")\n",
        "        # Create synthetic Iris data if URL fails\n",
        "        from sklearn.datasets import load_iris\n",
        "        iris = load_iris()\n",
        "        X = iris.data\n",
        "        y = np.array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])[iris.target]\n",
        "        return X, y, iris.feature_names, np.unique(y)\n",
        "\n",
        "def load_breast_cancer_coimbra_dataset():\n",
        "    \"\"\"Load Breast Cancer Coimbra dataset\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv\"\n",
        "    try:\n",
        "        response = urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(StringIO(data))\n",
        "        X = df.iloc[:, :-1].values  # Features\n",
        "        y = df.iloc[:, -1].values   # Classification column\n",
        "        # Convert class to strings for consistency\n",
        "        y = np.array(['Healthy' if label == 1 else 'Patient' for label in y])\n",
        "        feature_names = df.columns[:-1].tolist()\n",
        "        class_names = np.unique(y)\n",
        "        return X, y, feature_names, class_names\n",
        "    except:\n",
        "        print(\"Error loading Breast Cancer Coimbra dataset from URL. Generating synthetic data instead.\")\n",
        "        # Generate synthetic data if URL fails\n",
        "        np.random.seed(42)\n",
        "        n_samples = 116  # Actual dataset size\n",
        "        n_features = 9   # Actual number of features\n",
        "        X = np.random.randn(n_samples, n_features)\n",
        "        y = np.array(['Healthy' if i < 58 else 'Patient' for i in range(n_samples)])\n",
        "        feature_names = [\n",
        "            'Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin',\n",
        "            'Adiponectin', 'Resistin', 'MCP.1'\n",
        "        ]\n",
        "        class_names = np.unique(y)\n",
        "        return X, y, feature_names, class_names\n",
        "\n",
        "def load_ionosphere_dataset():\n",
        "    \"\"\"Load Ionosphere dataset\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\"\n",
        "    column_names = [f'feature_{i}' for i in range(34)] + ['class']\n",
        "    try:\n",
        "        response = urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(StringIO(data), header=None, names=column_names)\n",
        "        X = df.iloc[:, :-1].values\n",
        "        y = df.iloc[:, -1].values\n",
        "        feature_names = column_names[:-1]\n",
        "        class_names = np.unique(y)\n",
        "        return X, y, feature_names, class_names\n",
        "    except:\n",
        "        print(\"Error loading Ionosphere dataset from URL. Using synthetic data instead.\")\n",
        "        # Create synthetic ionosphere data if URL fails\n",
        "        X = np.random.randn(351, 34)\n",
        "        y = np.random.choice(['g', 'b'], size=351)\n",
        "        feature_names = [f'feature_{i}' for i in range(34)]\n",
        "        return X, y, feature_names, np.unique(y)\n",
        "\n",
        "def load_wine_dataset():\n",
        "    \"\"\"Load Wine dataset\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
        "    column_names = ['class', 'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
        "                   'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins',\n",
        "                   'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
        "    try:\n",
        "        response = urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(StringIO(data), header=None, names=column_names)\n",
        "        X = df.iloc[:, 1:].values  # Features\n",
        "        y = df.iloc[:, 0].values   # Class column\n",
        "        # Convert class to strings for consistency\n",
        "        y = np.array([f'Class_{int(label)}' for label in y])\n",
        "        feature_names = column_names[1:]\n",
        "        class_names = np.unique(y)\n",
        "        return X, y, feature_names, class_names\n",
        "    except:\n",
        "        print(\"Error loading Wine dataset from URL. Using synthetic data instead.\")\n",
        "        # Create synthetic wine data if URL fails\n",
        "        from sklearn.datasets import load_wine\n",
        "        wine = load_wine()\n",
        "        X = wine.data\n",
        "        y = np.array([f'Class_{i+1}' for i in wine.target])\n",
        "        return X, y, wine.feature_names, np.unique(y)\n",
        "\n",
        "def split_data(X, y, train_ratio=0.7, random_state=42):\n",
        "    \"\"\"\n",
        "    Split data into training and testing sets with similar class distributions\n",
        "\n",
        "    Parameters:\n",
        "    X: Features\n",
        "    y: Labels\n",
        "    train_ratio: Ratio of training data\n",
        "    random_state: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Get unique classes and their indices\n",
        "    classes = np.unique(y)\n",
        "    indices_per_class = [np.where(y == c)[0] for c in classes]\n",
        "\n",
        "    # Split indices for each class\n",
        "    train_indices = []\n",
        "    test_indices = []\n",
        "\n",
        "    for class_indices in indices_per_class:\n",
        "        np.random.shuffle(class_indices)\n",
        "        n_train = int(len(class_indices) * train_ratio)\n",
        "\n",
        "        train_indices.extend(class_indices[:n_train])\n",
        "        test_indices.extend(class_indices[n_train:])\n",
        "\n",
        "    # Get train/test splits\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def cross_validation(X, y, n_folds=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation\n",
        "\n",
        "    Parameters:\n",
        "    X: Features\n",
        "    y: Labels\n",
        "    classifier: Classifier object with train and predict methods\n",
        "    n_folds: Number of folds\n",
        "    random_state: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    X_train, X_test, y_train, y_test\n",
        "    #mean_accuracy: Mean accuracy across folds\n",
        "    #std_accuracy: Standard deviation of accuracy across folds\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Get indices for each class\n",
        "    classes = np.unique(y)\n",
        "    indices_per_class = [np.where(y == c)[0] for c in classes]\n",
        "\n",
        "    # Create stratified folds\n",
        "    fold_indices = [[] for _ in range(n_folds)]\n",
        "\n",
        "    for class_indices in indices_per_class:\n",
        "        np.random.shuffle(class_indices)\n",
        "        # Split class indices into n_folds parts\n",
        "        fold_size = len(class_indices) // n_folds\n",
        "\n",
        "        for fold_idx in range(n_folds):\n",
        "            start_idx = fold_idx * fold_size\n",
        "            end_idx = (fold_idx + 1) * fold_size if fold_idx < n_folds - 1 else len(class_indices)\n",
        "            fold_indices[fold_idx].extend(class_indices[start_idx:end_idx])\n",
        "\n",
        "    # Run cross-validation\n",
        "    accuracies = []\n",
        "\n",
        "    for test_fold in range(n_folds):\n",
        "        # Create train/test split\n",
        "        test_indices = fold_indices[test_fold]\n",
        "        train_indices = []\n",
        "        for fold_idx in range(n_folds):\n",
        "            if fold_idx != test_fold:\n",
        "                train_indices.extend(fold_indices[fold_idx])\n",
        "\n",
        "        X_train = X[train_indices]\n",
        "        y_train = y[train_indices]\n",
        "        X_test = X[test_indices]\n",
        "        y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "    \"\"\"\n",
        "        # Train and evaluate classifier\n",
        "        classifier.train(X_train, y_train)\n",
        "        y_pred, _ = classifier.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = np.mean(y_pred == y_test)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    return np.mean(accuracies), np.std(accuracies)\n",
        "    \"\"\"\n",
        "\n",
        "def normalize_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Normalize data using min-max scaling based on training data\n",
        "\n",
        "    Parameters:\n",
        "    X_train: Training features\n",
        "    X_test: Testing features\n",
        "\n",
        "    Returns:\n",
        "    X_train_norm: Normalized training features\n",
        "    X_test_norm: Normalized testing features\n",
        "    \"\"\"\n",
        "    # Convert boolean values to integers before normalization\n",
        "    X_train = X_train.astype(float)\n",
        "    X_test = X_test.astype(float)\n",
        "\n",
        "    # Calculate min and max values from training data\n",
        "    min_vals = np.min(X_train, axis=0)\n",
        "    max_vals = np.max(X_train, axis=0)\n",
        "    range_vals = max_vals - min_vals\n",
        "\n",
        "    # Avoid division by zero\n",
        "    range_vals[range_vals == 0] = 1\n",
        "\n",
        "    # Normalize\n",
        "    X_train_norm = (X_train - min_vals) / range_vals\n",
        "    X_test_norm = (X_test - min_vals) / range_vals\n",
        "\n",
        "    return X_train_norm, X_test_norm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def standardiize_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Normalize data using Z-score standardization based on training data\n",
        "\n",
        "    Parameters:\n",
        "    X_train: Training features\n",
        "    X_test: Testing features\n",
        "\n",
        "    Returns:\n",
        "    X_train_norm: Standardized training features\n",
        "    X_test_norm: Standardized testing features\n",
        "    \"\"\"\n",
        "    # Convert boolean values to floats before standardization\n",
        "    X_train = X_train.astype(float)\n",
        "    X_test = X_test.astype(float)\n",
        "\n",
        "    # Calculate mean and standard deviation from training data\n",
        "    mean_vals = np.mean(X_train, axis=0)\n",
        "    std_vals = np.std(X_train, axis=0)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    std_vals[std_vals == 0] = 1\n",
        "\n",
        "    # Standardize\n",
        "    X_train_norm = (X_train - mean_vals) / std_vals\n",
        "    X_test_norm = (X_test - mean_vals) / std_vals\n",
        "\n",
        "    return X_train_norm, X_test_norm\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Classifier Evaluation Program\")\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"\\nLoading datasets...\")\n",
        "\n",
        "\n",
        "    # Breast Cancer Coimbra dataset (binary)\n",
        "    X_bc, y_bc, feature_names_bc, class_names_bc = load_breast_cancer_coimbra_dataset()\n",
        "    print(f\"Breast Cancer Coimbra dataset loaded: {X_bc.shape[0]} samples, {X_bc.shape[1]} features, {len(class_names_bc)} classes\")\n",
        "\n",
        "\n",
        "    # Ionosphere (binary)\n",
        "    X_ion, y_ion, feature_names_ion, class_names_ion = load_ionosphere_dataset()\n",
        "    print(f\"Ionosphere dataset loaded: {X_ion.shape[0]} samples, {X_ion.shape[1]} features, {len(class_names_ion)} classes\")\n",
        "\n",
        "\n",
        "    # Iris dataset (multi-class)\n",
        "    X_iris, y_iris, feature_names_iris, class_names_iris = load_iris_dataset()\n",
        "    print(f\"Iris dataset loaded: {X_iris.shape[0]} samples, {X_iris.shape[1]} features, {len(class_names_iris)} classes\")\n",
        "\n",
        "    # Wine dataset (multi-class)\n",
        "    X_wine, y_wine, feature_names_wine, class_names_wine = load_wine_dataset()\n",
        "    print(f\"Wine dataset loaded: {X_wine.shape[0]} samples, {X_wine.shape[1]} features, {len(class_names_wine)} classes\")\n",
        "\n",
        "    # Process datasets\n",
        "    datasets = [\n",
        "        {\n",
        "            'name': 'Breast Cancer Coimbra',\n",
        "            'X': X_bc,\n",
        "            'y': y_bc,\n",
        "            'feature_names': feature_names_bc,\n",
        "            'class_names': class_names_bc,\n",
        "            'binary': True\n",
        "        },\n",
        "        {\n",
        "            'name': 'Ionosphere',\n",
        "            'X': X_ion,\n",
        "            'y': y_ion,\n",
        "            'feature_names': feature_names_ion,\n",
        "            'class_names': class_names_ion,\n",
        "            'binary': True\n",
        "        },\n",
        "        {\n",
        "            'name': 'Iris',\n",
        "            'X': X_iris,\n",
        "            'y': y_iris,\n",
        "            'feature_names': feature_names_iris,\n",
        "            'class_names': class_names_iris,\n",
        "            'binary': False\n",
        "        },\n",
        "        {\n",
        "            'name': 'Wine',\n",
        "            'X': X_wine,\n",
        "            'y': y_wine,\n",
        "            'feature_names': feature_names_wine,\n",
        "            'class_names': class_names_wine,\n",
        "            'binary': False\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Create classifier instances\n",
        "    classifiers = [\n",
        "        {\n",
        "            'name': 'Naive Bayes',\n",
        "            'model': NaiveBayesClassifier()\n",
        "        },\n",
        "        {\n",
        "            'name': 'Perceptron',\n",
        "            'model': PerceptronClassifier(learning_rate=0.01, n_iterations=1000)\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Create evaluator\n",
        "    evaluator = ClassifierEvaluator()\n",
        "\n",
        "    # Store ROC results\n",
        "    roc_results = []\n",
        "\n",
        "\n",
        "    # Process each dataset\n",
        "    for dataset in datasets:\n",
        "        print(f\"\\n\\n===== Dataset: {dataset['name']} =====\")\n",
        "\n",
        "        for i in range(0,3):\n",
        "          # Split data\n",
        "          X_train, X_test, y_train, y_test = split_data(dataset['X'], dataset['y'], train_ratio=0.7)\n",
        "          print(f\"Data split: \\n {X_train.shape[0]} training samples, {X_test.shape[0]} testing samples\")\n",
        "\n",
        "\n",
        "        #for k  in range(5,11):\n",
        "          # Cross-validation\n",
        "          #X_train, X_test, y_train, y_test = cross_validation(dataset['X'], dataset['y'], n_folds=k)\n",
        "          #print(f\"\\nCross-Validation ({k}-fold): \\n{X_train.shape[0]} training samples, {X_test.shape[0]} testing samples\")\n",
        "\n",
        "          # Without normalization\n",
        "          if (i==0):\n",
        "            print(f\"without normalization: \\n\")\n",
        "            X_train_norm = X_train\n",
        "            X_test_norm = X_test\n",
        "          # Normalize data\n",
        "          elif(i==1):\n",
        "            print(f\"with min-max: \\n\")\n",
        "            X_train_norm, X_test_norm = normalize_data(X_train, X_test)\n",
        "          else:\n",
        "            print(f\"with z-score: \\n\")\n",
        "            X_train_norm, X_test_norm = standardiize_data(X_train, X_test)\n",
        "\n",
        "          # Evaluate each classifier\n",
        "          for clf_info in classifiers:\n",
        "              classifier = clf_info['model']\n",
        "              clf_name = clf_info['name']\n",
        "\n",
        "              print(f\"\\n----- Classifier: {clf_name} -----\")\n",
        "\n",
        "              # Train classifier\n",
        "              classifier.train(X_train_norm, y_train)\n",
        "\n",
        "              # Test classifier\n",
        "              y_pred, discriminant_values = classifier.predict(X_test_norm)\n",
        "\n",
        "              # Compute confusion matrix\n",
        "              conf_matrix, classes = evaluator.compute_confusion_matrix(y_test, y_pred, dataset['class_names'])\n",
        "\n",
        "              # Compute metrics\n",
        "              metrics = evaluator.compute_metrics(conf_matrix)\n",
        "\n",
        "              # Display results\n",
        "              print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "              print(f\"Macro-Precision: {metrics['macro_precision']:.4f}\")\n",
        "              print(f\"Macro-Recall: {metrics['macro_recall']:.4f}\")\n",
        "              print(f\"Macro-F1: {metrics['macro_f1']:.4f}\")\n",
        "\n",
        "              # Display confusion matrix\n",
        "              print(\"\\nConfusion Matrix:\")\n",
        "              print(\" \" * 12, end=\"\")\n",
        "              for c in classes:\n",
        "                  print(f\"{c[:7]:>10}\", end=\"\")\n",
        "              print()\n",
        "\n",
        "              for i, c in enumerate(classes):\n",
        "                  print(f\"{c[:10]:>10} |\", end=\"\")\n",
        "                  for j in range(len(classes)):\n",
        "                      print(f\"{conf_matrix[i, j]:>10}\", end=\"\")\n",
        "                  print()\n",
        "\n",
        "\n",
        "              # Plot confusion matrix\n",
        "              evaluator.plot_confusion_matrix(conf_matrix, classes, title=f'{clf_name} Confusion Matrix - {dataset[\"name\"]}')\n",
        "\n",
        "              # For binary datasets, compute ROC curve\n",
        "              if dataset['binary']:\n",
        "                  # Convert class labels to binary (0 or 1)\n",
        "                  binary_labels = np.zeros(len(y_test))\n",
        "                  positive_class = dataset['class_names'][0]  # First class is positive\n",
        "                  binary_labels[y_test == positive_class] = 1\n",
        "\n",
        "                  # Get discriminant values for positive class\n",
        "                  if len(discriminant_values.shape) > 1 and discriminant_values.shape[1] > 1:\n",
        "                      # Multi-output discriminant (use first class)\n",
        "                      disc_positive = discriminant_values[:, 0]\n",
        "                  else:\n",
        "                      # Single output discriminant\n",
        "                      disc_positive = discriminant_values\n",
        "\n",
        "                  # Calculate and plot ROC curve\n",
        "                  fpr, tpr, auc = evaluator.calculate_roc_curve(binary_labels, disc_positive)\n",
        "                  print(f\"\\nAUC: {auc:.4f}\")\n",
        "\n",
        "                  # Store ROC results for later comparison\n",
        "                  if not hasattr(main, 'roc_results'):\n",
        "                      main.roc_results = []\n",
        "\n",
        "                  main.roc_results.append({\n",
        "                      'dataset': dataset['name'],\n",
        "                      'classifier': clf_name,\n",
        "                      'fpr': fpr,\n",
        "                      'tpr': tpr,\n",
        "                      'auc': auc\n",
        "                  })\n",
        "\n",
        "              # Plot ROC curves for binary dataset\n",
        "              if hasattr(main, 'roc_results'):\n",
        "                  plt.figure(figsize=(10, 8))\n",
        "\n",
        "                  for result in main.roc_results:\n",
        "                      label = f\"{result['classifier']} - {result['dataset']}\"\n",
        "                      evaluator.plot_roc_curve(result['fpr'], result['tpr'], result['auc'], label=label)\n",
        "\n",
        "                  plt.title('ROC Curves Comparison')\n",
        "                  plt.tight_layout()\n",
        "\n",
        "              # Show all plots\n",
        "              plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX-b1NkS8yIe",
        "outputId": "6fcd6fc7-afa9-4ec1-e4de-0a03eef3dfb9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Evaluation Program\n",
            "-----------------------------\n",
            "\n",
            "Loading datasets...\n",
            "Breast Cancer Coimbra dataset loaded: 116 samples, 9 features, 2 classes\n",
            "\n",
            "\n",
            "===== Dataset: Breast Cancer Coimbra =====\n",
            "Data split: \n",
            " 80 training samples, 36 testing samples\n",
            "without normalization: \n",
            "\n",
            "\n",
            "----- Classifier: Naive Bayes -----\n",
            "Accuracy: 0.6667\n",
            "Macro-Precision: 0.7385\n",
            "Macro-Recall: 0.6937\n",
            "Macro-F1: 0.6571\n",
            "\n",
            "Confusion Matrix:\n",
            "               Healthy   Patient\n",
            "   Healthy |        15         1\n",
            "   Patient |        11         9\n",
            "\n",
            "----- Classifier: Perceptron -----\n",
            "Accuracy: 0.5556\n",
            "Macro-Precision: 0.2778\n",
            "Macro-Recall: 0.5000\n",
            "Macro-F1: 0.3571\n",
            "\n",
            "Confusion Matrix:\n",
            "               Healthy   Patient\n",
            "   Healthy |         0        16\n",
            "   Patient |         0        20\n",
            "Data split: \n",
            " 80 training samples, 36 testing samples\n",
            "with min-max: \n",
            "\n",
            "\n",
            "----- Classifier: Naive Bayes -----\n",
            "Accuracy: 0.6667\n",
            "Macro-Precision: 0.7385\n",
            "Macro-Recall: 0.6937\n",
            "Macro-F1: 0.6571\n",
            "\n",
            "Confusion Matrix:\n",
            "               Healthy   Patient\n",
            "   Healthy |        15         1\n",
            "   Patient |        11         9\n",
            "\n",
            "----- Classifier: Perceptron -----\n",
            "Accuracy: 0.5278\n",
            "Macro-Precision: 0.4394\n",
            "Macro-Recall: 0.4813\n",
            "Macro-F1: 0.3923\n",
            "\n",
            "Confusion Matrix:\n",
            "               Healthy   Patient\n",
            "   Healthy |         1        15\n",
            "   Patient |         2        18\n",
            "Data split: \n",
            " 80 training samples, 36 testing samples\n",
            "with z-score: \n",
            "\n",
            "\n",
            "----- Classifier: Naive Bayes -----\n",
            "Accuracy: 0.6667\n",
            "Macro-Precision: 0.7385\n",
            "Macro-Recall: 0.6937\n",
            "Macro-F1: 0.6571\n",
            "\n",
            "Confusion Matrix:\n",
            "               Healthy   Patient\n",
            "   Healthy |        15         1\n",
            "   Patient |        11         9\n",
            "\n",
            "----- Classifier: Perceptron -----\n",
            "Accuracy: 0.6111\n",
            "Macro-Precision: 0.6333\n",
            "Macro-Recall: 0.5750\n",
            "Macro-F1: 0.5418\n",
            "\n",
            "Confusion Matrix:\n",
            "               Healthy   Patient\n",
            "   Healthy |         4        12\n",
            "   Patient |         2        18\n"
          ]
        }
      ]
    }
  ]
}